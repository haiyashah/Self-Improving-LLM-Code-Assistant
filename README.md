# Self-Improving LLM Code Assistant ğŸš€

## Project Goal
Building an intelligent AI agent that **reviews Python code, automatically detects logical errors, and continuously improves its reasoning** over time through robust verification and memory. This is designed as a **real-world developer assistant** to augment engineering workflows.

http://googleusercontent.com/image_generation_content/0



---

## Use Case: Revolutionizing Code Reviews ğŸ’¡
Large engineering teams dedicate thousands of hours to manual code reviews. Our agent streamlines this process by:
- **Analyzing Pull Requests (PRs):** Deeply understanding code changes.
- **Error Detection:** Proactively identifying logical, type, and edge-case errors.
- **Verified Recommendations:** Providing actionable feedback backed by programmatic checks.
- **Continuous Learning:** Reducing false positives and enhancing accuracy by learning from past mistakes.

http://googleusercontent.com/image_generation_content/1



---

## How It Works: The Iterative Improvement Loop ğŸ”„
Our agent operates on a powerful, self-improving cycle:

1.  **Plan:** The LLM generates structured, step-by-step review plans from PR diffs.
2.  **Execute:** The agent runs unit tests (`pytest`), performs static analysis (`pylint`, `mypy`), and executes other code checks.
3.  **Verify:** Each claim and proposed fix is programmatically confirmed against expected results and assertions.
4.  **Reflect:** Failed review steps, corrections, and valuable lessons learned are stored in a long-term memory.
5.  **Improve:** This stored memory is utilized in future reviews to avoid repeating errors and enhance reasoning capabilities.

http://googleusercontent.com/image_generation_content/2



---

## Technologies Under the Hood ğŸ› ï¸
A robust stack powers this intelligent assistant:

* **LLM:** GPT-4.1 for advanced planning and nuanced reasoning.
* **Tools:** Integrated with industry-standard tools like `pytest`, `mypy`, `pylint`, and `coverage.py` for comprehensive code analysis.
* **Memory:** Leverages FAISS for efficient similarity search and Sentence-Transformers for powerful embeddings of past experiences.
* **Infrastructure:** Built on Python 3.11 for performance, with Docker for consistent and reproducible environments.

http://googleusercontent.com/image_generation_content/3



---

## Evaluation Metrics ğŸ“ˆ
We rigorously track the agent's performance and improvement:

* **Verified Bug Precision (%):** How many detected bugs are genuinely correct and verified.
* **False Positive Rate (%):** The percentage of incorrect bug reports or recommendations.
* **Average Review Steps:** The efficiency of the agent's planning.
* **Improvement over successive runs:** Quantifying the benefits of the memory component.

---

## Results Snapshot: Agent with Memory vs. Baseline ğŸ“Š
Our results demonstrate significant improvements with the memory-enhanced agent:

| Metric                    | Baseline | Agent w/ Memory | Improvement |
|---------------------------|----------|-----------------|-------------|
| Verified Bug Precision    | 58%      | 84%             | **+26%** |
| False Positive Rate       | 31%      | 9%              | **-22%** |
| Avg. Review Steps         | 11.2     | 6.4             | **-4.8** |

http://googleusercontent.com/image_generation_content/4



---

## Project Structure: A Modular Design ğŸ›ï¸
Our agent is built with a clear, modular architecture for scalability and maintainability.

```text
.
â”œâ”€â”€ planner/                # LLM planning module (Generates structured review plans)
â”‚   â”œâ”€â”€ plan_generator.py   # Core logic for plan generation
â”‚   â”œâ”€â”€ plan_schema.json    # Defines JSON schema for structured plans
â”‚   â””â”€â”€ prompts/            # Optional prompt templates for LLM planning
â”œâ”€â”€ executor/               # Code execution and test runner (Executes code and runs checks)
â”‚   â”œâ”€â”€ run_tests.py        # Manages pytest and coverage.py execution
â”‚   â”œâ”€â”€ static_analysis.py  # Handles pylint and mypy checks
â”‚   â””â”€â”€ tool_interface.py   # Wrapper for programmatic tool calls
â”œâ”€â”€ verifier/               # Verification of plan steps (Confirms outputs and assertions)
â”‚   â”œâ”€â”€ verify_results.py   # Checks if step outputs match expected results
â”‚   â”œâ”€â”€ assertion_checks.py # Encodes deterministic checks per step
â”‚   â””â”€â”€ verification_utils.py # Helpers for logging and reporting verification
â”œâ”€â”€ memory/                 # Stores past failures and corrections (Learns from experience)
â”‚   â”œâ”€â”€ memory_db.py        # FAISS vector DB interface for storing memories
â”‚   â”œâ”€â”€ embeddings.py       # Manages embeddings of failed steps / lessons
â”‚   â””â”€â”€ memory_utils.py     # Utilities to query/update memory for future plans
â”œâ”€â”€ evaluation/             # Metrics and analysis scripts (Assesses performance and improvement)
â”‚   â”œâ”€â”€ evaluate_agent.py   # Calculates precision, false positives, steps
â”‚   â”œâ”€â”€ plot_metrics.py     # Generates plots of improvement over time
â”‚   â””â”€â”€ ablation_studies.py # Optional experiments to study component impact
â”œâ”€â”€ data/                   # Sample pull requests and test data (Input for the agent)
â”‚   â”œâ”€â”€ sample_prs/         # Directory of Python PR diffs
â”‚   â”œâ”€â”€ tests/              # Unit tests for the PRs
â”‚   â””â”€â”€ configs.json        # Mapping PRs to expected outputs
â”œâ”€â”€ configs/                # Configuration files (Agent and evaluation settings)
â”‚   â”œâ”€â”€ agent_config.yaml   # LLM temperature, max tokens, allowed tools
â”‚   â””â”€â”€ evaluation_config.yaml # Metrics and evaluation settings
â””â”€â”€ README.md               # Project overview

```

By: Haiya Shah
